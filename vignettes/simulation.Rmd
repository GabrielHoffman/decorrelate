
---
title: "Decorrelate simulations"
subtitle: ''
author: "Developed by [Gabriel Hoffman](http://gabrielhoffman.github.io/)"
date: "Run on `r Sys.time()`"
documentclass: article
output: 
  html_document:
  toc: true
  smart: false
vignette: >
  %\VignetteIndexEntry{Decorrelate}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\usepackage[utf8]{inputenc}
---


<!--- 

rmarkdown::render("simulation.Rmd");

--->



```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning=FALSE,
  message=FALSE,
  error = FALSE,
  tidy = FALSE,
  eval = FALSE,
  dev = c("png", "pdf"),
  cache = TRUE)
```

```{r load.packages, cache=FALSE}
suppressPackageStartupMessages({
library(decorrelate)
library(Matrix)
library(Rfast)
library(ggplot2)
library(latex2exp)
library(cowplot)
library(corrplot)
library(viridis)
library(vegan)
library(corpcor)
library(nlshrink)
library(ShrinkCovMat)
library(TAS)
library(beam)
library(corpcor)
library(ShrinkCovMat)
library(CovTools)
})

# Create correlation matrix with autocorrelation
autocorr.mat <- function(p = 100, rho = 0.9) {
    mat <- diag(p)
    return(rho^abs(row(mat)-col(mat)))
}

# make scatter plot with 2D density using viridis colors
plotScatterDensity = function(value1, value2){

  # Compute two dimensional density
  get_density <- function(x, y, n = 250) {
    dens <- MASS::kde2d(x = x, y = y, n = n)
    ix <- findInterval(x, dens$x)
    iy <- findInterval(y, dens$y)
    ii <- cbind(ix, iy)
    return(dens$z[ii])
  }

  # convert two vectors to a data frame
  df = data.frame(cbind(value1, value2))

  # determine limits of the plot
  lim = with(df, max(abs(c(value1, value2))))

  # Compute 2D density
  df$density <- get_density(df$value1, df$value2, n = 100)

  # Scatter plot colored by density
  ggplot(df, aes(value1, value2, color=density)) + geom_point(size=.4) + theme_classic(16) + theme(aspect.ratio=1, legend.position="bottom", plot.title = element_text(hjust = 0.5)) + geom_abline(color="red") + geom_vline(xintercept=0, col="grey40", linetype="dashed") + geom_hline(yintercept=0, col="grey40", linetype="dashed") + xlim(-lim, lim) + ylim(-lim, lim) + scale_color_viridis() + geom_smooth(method="gam", se=FALSE, color="darkorange")
}

tr = function(x) sum(diag(x))

minvsqrt = function(S){
  dcmp = eigen(S)
  with(dcmp, vectors %*% diag(1/sqrt(values)) %*% t(vectors))
}


msqrt = function(S){
  dcmp = eigen(S)
  with(dcmp, vectors %*% diag(sqrt(values)) %*% t(vectors))
}

# lowest eigen values
omega = function(C) min(eigen(C)$values)


# largest eigen values
alpha = function(C) max(eigen(C)$values)


data(varespec)
vare.dist <- vegdist(wisconsin(varespec))
mds.null <- monoMDS(vare.dist, y = cmdscale(vare.dist))
mds.alt <- monoMDS(vare.dist)
# scale = TRUE, symmetric = FALSE
vare.proc <- procrustes(mds.alt, mds.null)
vare.proc$ss

# Compute sum of squares for procrustes analysis
# Rotate Y to match X
procrustes_ss = function(X, Y, symmetric = FALSE){

  # scale = TRUE, 

  ctrace <- function(MAT) sum(MAT^2)
  X <- scale(X, scale = FALSE)
  Y <- scale(Y, scale = FALSE)
  if( symmetric ){
    X <- X/sqrt(ctrace(X))
    Y <- Y/sqrt(ctrace(Y))
  }

  # sol <- svd(crossprod(X, Y), nu=0, nv=0)

  k = min(c(dim(X), dim(Y)))
  sol = irlba::svdr(crossprod(X, Y), k)
  sum_d = sum(sol$d)
  # sum_d = sum(sqrt(eigen(crossprod(crossprod(X, Y)), symmetric=TRUE,only.values=TRUE)$values)[1:k])

  c <- sum_d/ctrace(Y)

  R2 <- ctrace(X) + c * c * ctrace(Y) - 2 * c * sum_d
  R2
}

# sol = irlba::svdr(crossprod(X, Y), k)
# sum(sol$d)
# sol$d[1:3]



# sum(svd(crossprod(X,Y))$d)

# sqrt(eigen(crossprod(tcrossprod(X,Y)))$values[1:3])


# k = min(c(dim(X), dim(Y)))
# system.time({
#   sol = irlba::svdr(crossprod(X, Y), k)
# })
# sum(sol$d)

# system.time({
#   sum(sqrt(eigen(crossprod(crossprod(X, Y)), symmetric=TRUE,only.values=TRUE)$values)[1:k])
# })


# system.time({
#   sum(sqrt(irlba::partial_eigen(crossprod(crossprod(X, Y)), k+1, symmetric=TRUE,nu=0)$values))
# })


# X = scores(mds.alt)
# Y = scores(mds.null)
# procrustes(X, Y, symmetric=TRUE)$ss
# procrustes_ss(X, Y, TRUE)
# procrustes(X, Y, symmetric=FALSE)$ss 
# procrustes_ss(X,Y, FALSE)


# # system.time({
# #   a <- procrustes(X_latent, X_est, symmetric=TRUE)$ss
# #   })


# # system.time({
# #   b <- procrustes_ss(X_latent, X_est) 
# #   })

# X = X_latent
# Y = X_est

# res = procrustes(X, Y, symmetric=FALSE)


```



# Test statistical performance of decorrelate 

```{r decorrelate.test1, cache=FALSE, fig.width=13, fig.height=6}
set.seed(12)
n = 300 # number of samples
p = 12000 # number of features per block

# create correlation matrix
Sigma = autocorr.mat(p, .95) * 1

X_latent = scale(matrnorm(n,p))

# sample matrix from MVN with covariance Sigma
Y = X_latent %*% msqrt(Sigma) + matrnorm(n,p) / 1
# Y = scale(Y)

# Estimate correlation with low rank and shrinkage
ecl_cov = eclairs( Y, compute="covariance")
ecl_corr = eclairs( Y, compute="correlation")

#, k=n-1)
# ecl_cov$dSq = ecl_cov$dSq + 1e-5
# ecl_corr$dSq = ecl_corr$dSq + 1e-5


# EB is too close to zero with small n
# CovEst methods are quadratic time
target = diag(mean(apply(Y, 2, var)), ncol(Y))

lambda_min = 0#ifelse(n > p, 0, 1e-3)
lambda_estimates = c(ShrinkCovMat = mvIC:::shrinkcovmat.equal_lambda(t(Y))$lambda_hat,
                    corpcor = estimate.lambda(scale(Y), verbose=FALSE),
                    EB = ecl_cov$lambda)
# lambda_estimates = c(lambda_estimates,
#                     LW_2003 = CovEst.2003LW(Y, target)$delta,
#                     OAS_2010 = CovEst.2010OAS(scale(Y))$rho,
#                     RBLW = CovEst.2010RBLW(scale(Y))$rho)




lambda_values = c(ecl_corr$lambda, ecl_cov$lambda, lambda_estimates, seq(lambda_min, .9, length.out=20))
lambda_values = sort(unique(lambda_values))

df = lapply( lambda_values, function(lambda){

  # Evaluate the correlation remaining after applying ADT
  C_decor = decorrelate(decorrelate(Sigma, ecl_corr, lambda=lambda), ecl_corr, lambda=lambda, transpose=TRUE)
  norm_decor = norm( C_decor - diag(ecl_cov$nu, ncol(C_decor)), "F")^2 / length(C_decor)

  norm_cov = norm(Sigma - getCor(ecl_corr, lambda), "F")^2 / length(Sigma)

  # Original data
  # mean squared error
  # mean((X_latent- X_est)^2)
  X_est = decorrelate(Y, ecl_cov, lambda=lambda)
  # X_est = scale(X_est, center=FALSE)
  # norm_data = norm(X_latent - X_est, "F")^2 / length(X_latent)

  # mean squared error after rotation
  # res = procrustes(X_latent, X_est, symmetric=TRUE)
  # norm_data = res$ss / length(X_latent)
  # res_pr = procrustes(X_latent, X_est)
  # norm(X_latent - res_pr$Yrot, "F")^2 / length(X_latent)
  norm_data = procrustes_ss(X_latent, X_est) / length(X_latent)

  data.frame( lambda = lambda, 
    norm_cov = norm_cov,
    norm_decor = norm_decor, 
    norm_data = norm_data )
  })
df = do.call(rbind, df)


df[1,]
df[df$lambda %in% c(Zero=0,lambda_estimates),]

mse_zero = df$norm_data[1]
mse = df[df$lambda == lambda_estimates['EB'],'norm_data'][1]
fracLess = (mse - mse_zero) / mse_zero


fig1 = ggplot(df, aes(lambda, norm_cov)) + geom_point() + theme_classic() + xlab(TeX("$\\lambda$")) + ylab("Mean squared error") + theme(aspect.ratio=1, plot.title = element_text(hjust = 0.5), legend.position="none") + ggtitle("Estimate covariance matrix") + xlim(0,1) 

df_lambda = data.frame(Method = names(lambda_estimates), lambda = lambda_estimates)
df_lambda$Method = factor(df_lambda$Method, df_lambda$Method)
fig1 = fig1 + geom_vline(data=df_lambda, aes(xintercept=lambda, color=Method), size=1.4) + scale_color_brewer(palette="Set1")

figB = ggplot(df[-1,], aes(lambda, norm_decor)) + geom_point() + theme_classic() + xlab(TeX("$\\lambda$")) + ylab("Mean squared error") + theme(aspect.ratio=1, plot.title = element_text(hjust = 0.5), legend.position="none") + ggtitle("Removing correlation") + xlim(0,1) 

df_lambda = data.frame(Method = names(lambda_estimates), lambda = lambda_estimates)
df_lambda$Method = factor(df_lambda$Method, df_lambda$Method)
figB = figB + geom_vline(data=df_lambda, aes(xintercept=lambda, color=Method), size=1.4) + scale_color_brewer(palette="Set1")

fig2 = ggplot(df, aes(lambda, norm_data)) + geom_point() + theme_classic() + xlab(TeX("$\\lambda$")) + ylab("Mean squared error") + theme(aspect.ratio=1, plot.title = element_text(hjust = 0.5)) + ggtitle("Recovery of independent signal") + xlim(0,1) 

df_lambda = data.frame(Method = names(lambda_estimates), lambda = lambda_estimates)
df_lambda$Method = factor(df_lambda$Method, df_lambda$Method)
fig2 = fig2 + geom_vline(data=df_lambda, aes(xintercept=lambda, color=Method), size=1.4) + scale_color_brewer(palette="Set1")

# # oracle loss
# X_est = Y %*% minvsqrt(Sigma)
# res = procrustes(X_latent, X_est)
# mse_oracle = res$ss / length(X_latent)
# fig2 = fig2 + geom_hline(yintercept = mse_oracle, linetype="dashed")

plot_grid(fig1, figB, fig2, nrow=1, align="vh", axis="tlbr")
```

Reduce MSE by r format(-fracLess*100, digits=4)%







<!---
http://www.matrixcalculus.org

tr( AB * diag(1/((1-lambda)*d + lambda*v))) - 2 *tr( C * diag(1/((1-lambda)*d + lambda*v)^(1/2)))



set.seed(12)
n = 100 # number of samples
p = 5 # number of features per block

# create correlation matrix
Sigma = autocorr.mat(p, .95) * 4

X = matrnorm(n,p)

tr = function(x) sum(diag(x))

U = eigen(Sigma)$vectors
dSq = eigen(Sigma)$values



library(Deriv)
f = function(lambda){
  a = sum(diag( t(U) %*% Sigma %*% U %*% diag(1 / ( (1-lambda)*dSq + lambda*v))))
  b = sum(diag( t(U) %*% chol(Sigma) %*% crossprod(X) %*% chol(Sigma) %*% U %*% diag(1 / ( (1-lambda)*dSq + lambda*v))))
  c = sum(diag( t(U) %*% t(X) %*% X %*% chol(Sigma)%*% U %*% diag(1 / sqrt( (1-lambda)*dSq + lambda*v))))
  a+ b - 2*c
}
f_prime = Deriv(f, "lambda", nderiv=1) 
f_prime_prime = Deriv(f, "lambda", nderiv=2) 


v = 4
x = seq(0, 1, length.out=1000)

par(mfrow=c(3,1))
plot(x, sapply(x, f))
plot(x, sapply(x, f_prime))
plot(x, sapply(x, f_prime_prime))


min(sapply(x, f_prime_prime))
--->





Show that the relative improvement increases as p approaches n.

I can prove better construction error for n > p.

For p >> n, true with adding epsilon.  But no way to estimate 

Can add epsilon to the eigen-values, but 
1) is not motivated by first principles
2) performance can be very sensitive to value of epsilon
3) no good way to select epsilon other then expensive cross-validation

EB method estimates lambda based on n, p, dSq

```{r test2, eval=FALSE}

# Estimate correlation with low rank and shrinkage
ecl_cov = eclairs( Y, compute="covariance")
ecl_corr = eclairs( Y, compute="correlation")

lambda = 0

eps_values = 10^-seq(-3, 7, length.out=20)

df_eps = lapply( eps_values, function(epsilon){

  ecl1 = ecl_cov
  ecl2 = ecl_corr

  ecl1$dSq = ecl1$dSq + epsilon
  ecl2$dSq = ecl2$dSq + epsilon

  # Evaluate the correlation remaining after applying ADT
  C_decor = decorrelate(decorrelate(Sigma, ecl2, lambda=lambda), ecl2, lambda=lambda, transpose=TRUE)
  norm_decor = norm( C_decor - diag(ecl1$nu, ncol(C_decor)), "F")^2 / length(C_decor)

  # Original data
  # mean squared error
  # mean((X_latent- X_est)^2)
  X_est = decorrelate(Y, ecl1, lambda=lambda)
  norm_data = norm(X_latent - X_est, "F")^2 / length(X_latent)

  data.frame( epsilon = epsilon, norm_decor = norm_decor, norm_data = norm_data )
  })
df_eps = do.call(rbind, df_eps)

# Symbolic differentiation

library(Deriv)

f = function(lambda){
  sum(1 / ( (1-lambda)*d^2 + lambda*v))
}
f_prime = Deriv(f, "lambda", nderiv=1) 
# -(d^2* (v - d^2)/(d^2 * (1 - lambda) + lambda * v)^2); 

f_prime_prime = Deriv(f, "lambda", nderiv=2)
# 2 * ( d^2 * (v -  d^2)^2/( d^2 * (1 - lambda) + lambda * v)^3)

v = 4.1
d = c(3, 3, 1)
x = seq(0, 1, length.out=1000)

par(mfrow=c(3,1))
plot(x, sapply(x, f))
plot(x, sapply(x, f_prime))
plot(x, sapply(x, f_prime_prime))



f = function(lambda){
  sqrt(g(f(lambda))
}
Deriv(f, "lambda", nderiv=1) 
```



f = function(lambda){
  2 / ( (1-lambda)*dSq + lambda*v)
}
Deriv(f, "lambda", nderiv=1)
Deriv(f, "lambda", nderiv=2) 


```{r test}
# Cases
# p > n
#   full spectrum or, add epsilon
#    with small lambda, lambda^alpha is unstable
#     But so is pseudoinverse
# p < p

# 1) Metric of independence
# depends on n vs p, and truncation, epsilon
# MLE is best estimator of Sigma and C_decor because this is a funtion of Sigma

# 2) independent signal
# But lambda is a better estimate of the independent signal

# Note that decorrelate with lambda = 0 is different from MLE due to numerical reasons



  # L = chol(solve(cora(Y) + diag(lambda, ncol(Y))))
  # norm(X_latent - Y %*% L, "F")




# DOI: 10.1016/j.csda.2018.03.010
prior_jeffries = function(nu, p){
  v1 = trigamma(nu/2) - trigamma((nu+p)/2) - 2*p*(nu+p+4)/(nu*(nu+p)*(nu+p+2))
  sqrt(v1)
}

p = 10
nu_array = seq(3, 25000, length.out=100)

df_prior = lapply(nu_array, function(nu){
  data.frame(nu = nu, prior = log(prior_jeffries(nu,p)))
  })
df_prior = do.call(rbind, df_prior)

plot(df_prior)


p = 1000
df_prior = lapply(nu_array, function(nu){
  data.frame(nu = nu, prior = log(prior_jeffries(nu,p)))
  })
df_prior = do.call(rbind, df_prior)

lines(df_prior)



f = function (delta, p, n, eigs, logdetD) {
    out = -0.5 * n * p * log(pi)
    out = out + lmvgamma((delta + n) * 0.5, p)
    out = out - lmvgamma(delta * 0.5, p)
    out = out + 0.5 * delta * p * log(delta - p - 1)
    if (n > p) {
        out = out - 0.5 * (delta + n) * sum(log((delta - p - 
            1) + eigs))
    }
    else {
        out = out - 0.5 * (delta + n) * (sum(log((delta - p - 
            1) + eigs)) + (p - length(eigs)) * sum(log(delta - 
            p - 1)))
    }
    out = out - 0.5 * n * logdetD
    out
}

g = function(delta, p, n, eigs, logdetD){
  ll = f(delta, p, n, eigs, logdetD) 
  # ll = ll - 2*log(delta)
  ll = ll #+ log(prior_jeffries(delta, p))
}

assignInNamespace('logML', g, "decorrelate")

decorrelate:::logML



lambda_values = seq(0, 1, length.out=100)
df_logML = lapply(lambda_values, function(lambda){
  ecl = eclairs( Y[1:10,1:100], compute="covariance", lambda=lambda)
  data.frame(lambda = lambda, logML = ecl$logML)
  })
df_logML = do.call(rbind, df_logML)

ggplot(df_logML, aes(lambda, logML)) + geom_point() + theme_classic()


eclairs( Y[1:10,], compute="covariance")











```






