
---
title: "Decorrelate"
subtitle: ''
author: "Developed by [Gabriel Hoffman](http://gabrielhoffman.github.io/)"
date: "Run on `r Sys.time()`"
documentclass: article
output: 
  html_document:
  toc: true
  smart: false
vignette: >
  %\VignetteIndexEntry{Decorrelate}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\usepackage[utf8]{inputenc}
---


<!--- 

rmarkdown::render("decorrelate.Rmd");

--->



```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning=FALSE,
  message=FALSE,
  error = FALSE,
  tidy = FALSE,
  dev = c("png", "pdf"),
  cache = TRUE)
```

```{r load.packages}
library(decorrelate)
library(Matrix)
library(Rfast)
library(ggplot2)
library(cowplot)
library(corrplot)
library(viridis)

# Create correlation matrix with autocorrelation
autocorr.mat <- function(p = 100, rho = 0.9) {
    mat <- diag(p)
    return(rho^abs(row(mat)-col(mat)))
}

# make scatter plot with 2D density using viridis colors
plotScatterDensity = function(value1, value2){

  # Compute two dimensional density
  get_density <- function(x, y, n = 250) {
    dens <- MASS::kde2d(x = x, y = y, n = n)
    ix <- findInterval(x, dens$x)
    iy <- findInterval(y, dens$y)
    ii <- cbind(ix, iy)
    return(dens$z[ii])
  }

  # convert two vectors to a data frame
  df = data.frame(cbind(value1, value2))

  # determine limits of the plot
  lim = with(df, max(abs(c(value1, value2))))

  # Compute 2D density
  df$density <- get_density(df$value1, df$value2, n = 100)

  # Scatter plot colored by density
  ggplot(df, aes(value1, value2, color=density)) + geom_point(size=.4) + theme_classic(16) + theme(aspect.ratio=1, legend.position="bottom", plot.title = element_text(hjust = 0.5)) + geom_abline(color="red") + geom_vline(xintercept=0, col="grey40", linetype="dashed") + geom_hline(yintercept=0, col="grey40", linetype="dashed") + xlim(-lim, lim) + ylim(-lim, lim) + scale_color_viridis() + geom_smooth(method="gam", se=FALSE, color="darkorange")
}
```



# Test statistical performance of decorrelate 
## p >> n
```{r decorrelate.test1}
set.seed(1)
n = 500 # number of samples
p = 500 # number of features per block
n_blocks = 2^5

# create correlation matrix
Sigma = autocorr.mat(p, .95)
for( i in 1:log2(n_blocks)){
    Sigma = bdiag(Sigma, Sigma)
}

# sample matrix from MVN with covariance Sigma
Y = rmvnorm(n, rep(0, ncol(Sigma)), sigma=Sigma)

# Estimate correlation with low rank and shrinkage
ecl = eclairs(Y)

# Approximate decorrelation transformation using eclairs
Y.decorr = decorrelate(Y, ecl)
```


```{r test.2param}

# standard
Y.decorr = decorrelate(Y, ecl)

# under the hood
Y.decorr2 = mult_eclairs(Y, ecl$U, ecl$dSq, 
        ecl$lambda, nu = ecl$nu, alpha = -1/2, transpose = FALSE)

mean(abs(Y.decorr - Y.decorr2))

# # more flexible function
# # beta = lambda
# # gamma = (1-lambda)
# mult_eclairs_v2 = function (X, U1, dSq1, beta, gamma, nu, alpha, transpose = FALSE) {
#     v = dSq1 * gamma + beta * nu
#     if (transpose) {
#         if (nrow(X) != nrow(U1)) {
#             stop("Incompatable dimensions:\nData matrix X has ", 
#                 nrow(X), " columns, but low rank component has ", 
#                 nrow(U1), " rows")
#         }
#         X_U1 = crossprod(X, U1)
#         part1 = 0
#         if (beta > 0) {
#             part1 = ((X - tcrossprod(U1, X_U1)) * ((beta * 
#                 nu)^alpha))
#         }
#         result = crossprod((v^alpha) * t(U1), t(X_U1)) + part1
#     }
#     else {
#         if (ncol(X) != nrow(U1)) {
#             stop("Incompatable dimensions:\nData matrix X has ", 
#                 ncol(X), " columns, but low rank component has ", 
#                 nrow(U1), " rows")
#         }
#         X_U1 = X %*% U1
#         part1 = 0
#         if (beta > 0) {
#             part1 = (X - tcrossprod(X_U1, U1)) * ((beta * nu)^alpha)
#         }
#         result = X_U1 %*% ((v^alpha) * t(U1)) + part1
#     }
#     result
# }


# Y.decorr3 = mult_eclairs_v2(Y, ecl$U, ecl$dSq, 
#         beta = ecl$lambda, gamma = 1 - ecl$lambda,
#         nu = ecl$nu, alpha = -1/2, transpose = FALSE)

# mean(abs(Y.decorr - Y.decorr3))




# two parameter posterior expected precision
############################################

# For 2 parameter posterior expectation of precision
deltaTo2Param = function(delta, n, p){
  list(beta = (delta -p -1)/(delta + n),
      gamma  = n / (delta + n))
}

delta = with(ecl, decorrelate:::alphaToDelta(lambda, n, p))

res = deltaTo2Param(delta, ecl$n, ecl$p)
res


prec_scale = with(ecl, (delta+n)/(delta+n-p-1))
prec_scale


Y.decorr4 = mult_eclairs_v2(Y, ecl$U, ecl$dSq, 
        beta = res$beta, gamma = res$gamma,
        nu = ecl$nu, alpha = -1/2, transpose = FALSE)  / sqrt(prec_scale)

C_decor = decorrelate(decorrelate(Sigma, ecl)  * prec_scale, ecl, transpose=TRUE)  * prec_scale

Sig.prec = mult_eclairs_v2(Sigma, ecl$U, ecl$dSq, 
        beta = res$beta, gamma = res$gamma,
        nu = ecl$nu, alpha = -1/2, transpose = FALSE) / sqrt(prec_scale)
Sig.prec = mult_eclairs_v2(Sig.prec, ecl$U, ecl$dSq, 
        beta = res$beta, gamma = res$gamma,
        nu = ecl$nu, alpha = -1/2, transpose = TRUE) / sqrt(prec_scale)

Sig.prec[1:4, 1:4]


# Sig.prec = mult_eclairs_v2(Sigma, ecl$U, ecl$dSq, 
#         beta = ecl$lambda, gamma = 1-ecl$lambda,
#         nu = ecl$nu, alpha = -1/2, transpose = FALSE)
# Sig.prec = mult_eclairs_v2(Sig.prec, ecl$U, ecl$dSq, 
#         beta = ecl$lambda, gamma = 1-ecl$lambda,
#         nu = ecl$nu, alpha = -1/2, transpose = TRUE)


# standard
Y.decorr4 = decorrelate(Y, ecl)  / sqrt(prec_scale)



k = 100
A = cova(Y[,1:k])^2
B = cova(Y.decorr[,1:k])^2
C = cova(Y.decorr4[,1:k])^2

a = A[lower.tri(A)]
b = B[lower.tri(B)]
c = C[lower.tri(C)]

par(mfrow=c(1,3))
plot(a,b, main="Inverse of posterior expectation", xlim=c(0, 1), ylim=c(0,1))
abline(0, 1, col="red")

plot(a,c, main="Posterior expected precision", xlim=c(0, 1), ylim=c(0,1))
abline(0, 1, col="red")

plot(b,c, main="Cov vs prec")
abline(0, 1, col="red")




a = Sigma[lower.tri(Sigma)]
b = C_decor[lower.tri(C_decor)] 
c = Sig.prec[lower.tri(Sig.prec)] 

plot(a[1:10000], b[1:10000], xlim=c(0,1), ylim=c(0, max(c)))
plot(a[1:10000], c[1:10000], xlim=c(0,1), ylim=c(0, max(c)))


Sig_inv = drop0(solve(Sigma), 1e-8)
a = Sig_inv[lower.tri(Sig_inv, diag=TRUE)]
b = C_decor[lower.tri(C_decor, diag=TRUE)] 
c = Sig.prec[lower.tri(Sig.prec, diag=TRUE)] 

plot(a[1:10000], b[1:10000])
plot(a[1:10000], c[1:10000])





```




eclairs_corMat(Sigma, n)

eclairs(Y[,1:p])

eclairs(Y[,1:(10*p)])

# GEH: June 1, 2021
# Create local LD
# Create separate clusters
# eclairs decomp on each clusters
# run decorrelate on list of eclairs decomp

adjacencyClustering = function(Csq, window, k){

  # Adjacency-constrained clustering
  hcl <- adjClustFast(Csq, "similarity", h = window)

  # number of clusters
  cutree_chac(correct(hcl), k=k)
}



library(adjClustFast)

# create correlation matrix
Csq = cora(Y)^2
window = 1000

if( ! is(Csq, 'sparseMatrix') ){
  # convert to sparseMatrix with small entries dropped
  Csq = as(Csq, "sparseMatrix")
}
Csq = drop0(Csq, tol=0.02)
Csq = as(Csq, "symmetricMatrix")


# identify clusters
# matL and matR memory usage prevents wider windows
clusters = adjacencyClustering(Csq, window, 30)

# perform eclairs decomposition on each cluster
eclList = lapply(seq(1, max(clusters)), function(i){

  idx = which(clusters == i)
  eclairs_corMat(Csq[idx,idx], n)
  })

dc = lapply(seq(1, max(clusters)), function(i){
  idx = which(clusters == i)
  decorrelate(Y[,idx,drop=FALSE], eclList[[i]])
  })


Y_decorr = do.call(cbind, dc)

A = cora(Y[,1:1000])^2
B = cora(Y_decorr[,1:1000])^2

a = A[lower.tri(A)]
b = B[lower.tri(B)]

par(mfrow=c(1,2))
plot(a,b, main="clustered", xlim=c(0, 1), ylim=c(0,1))
abline(0, 1, col="red")

hist( sapply(eclList, function(x) x$lambda), xlim=c(0,1))



ecl = eclairs(Y)
Y_decorr_full = decorrelate(Y, ecl)

A = cora(Y[,1:1000])^2
B = cora(Y_decorr_full[,1:1000])^2

a = A[lower.tri(A)]
b = B[lower.tri(B)]

plot(a,b, main="full", xlim=c(0, 1), ylim=c(0,1))
abline(0, 1, col="red")



i = 450:550
C = Csq[i,i]
image(C)
hcl <- adjClustFast(C, "similarity", h = as.integer(length(i)/2))
plot(hcl)

cutree_chac(hcl, h=15)




i = 1:550
C = Csq[i,i]
image(C)
hcl <- adjClustFast(C, "similarity", h = as.integer(length(i)/2))
plot(hcl)




```{r show.plots, fig.width=7, fig.height=7.5}
par(mfrow=c(2,2))
n_features = 300
i = 1:n_features

corrplot(as.matrix(Sigma[i,i]), main="True correlation matrix", method="color", tl.pos='n', mar=c(0,0,1,0))
# corrplot(getCor(ecl)[i,i], main="Estimated correlation matrix", method="color", tl.pos='n', mar=c(0,0,1,0))
corrplot(cor(Y.decorr[,i]), main="Correlation after ADT", method="color", tl.pos='n', mar=c(0,0,1,0))
plot(ecl)

ev_shrunk = with(ecl, (1-lambda)*dSq + nu*lambda)
ymax = max(ecl$dSq)
plot(ecl$dSq, ev_shrunk, xlim=c(0, ymax), ylim=c(0, ymax), xlab="Eigen-values (observed)", ylab="Eigen-values (shrinkages)", main="Comparison of eigen-values")
abline(0,1, col="red")
```

# eigs = pinnacle:::optimal_sv_shrinkage( sqrt(ecl$dSq), ecl$n, ecl$p)^2
# plot(ecl$dSq)
# points(eigs, col="red")

# estimate_lambda_eb(n * ecl$dSq, ecl$n, ecl$p, ecl$nu)


# estimate_lambda_eb(n * eigs, ecl$n, ecl$p, ecl$nu)



# eigs = pinnacle:::optimal_sv_shrinkage( sqrt(n*ecl$dSq), ecl$n, ecl$p)^2
# estimate_lambda_eb(eigs, ecl$n, ecl$p, ecl$nu)






```{r corr.subet, fig.width=5, fig.height=5}
# Evaluate correlatilon between a subset of features
n_features = ncol(Y)
ecl2 = eclairs(Y[,1:n_features], lambda=ecl$lambda)

# Evaluate the correlation remaining after applying ADT
C_decor = decorrelate(decorrelate(Sigma[1:n_features,1:n_features], ecl2), ecl2, transpose=TRUE)

# Sample correlation
C_sample = cora(Y[,1:n_features])

# extract correlation values
C_decor.array = C_decor[lower.tri(C_decor)]
C_sample.array = C_sample[lower.tri(C_sample)]

# fig1 = plotScatterDensity( abs(a),abs(b)) + xlim(0, 1) + ylim(0,1) + xlab("Sample correlation") + ylab("Correlation after transformation") + scale_x_continuous(expand=c(0, 0), limits=c(0, 1)) + scale_y_continuous(expand=c(0, 0), limits=c(0, 1))
# plot_grid(fig1, fig2, nrow=1)
 
i = 1:10000
plotScatterDensity( C_sample.array[i], C_decor.array[i] ) + xlim(NA, 1) + ylim(-NA,1) + xlab("Sample correlation") + ylab("Correlation after ADT")  
```

```{r plot.density, fig.width=5, fig.height=4}
df = rbind( data.frame(Correlation = C_sample.array, Method = "Sample correlation"),
            data.frame(Correlation = C_decor.array, Method = "Correlation after ADT"))

df$Method = factor(df$Method, unique(df$Method))

ggplot(df, aes(Correlation, fill=Method, color=Method)) + geom_density(alpha = .1) + theme_classic() + geom_vline(xintercept=0, color="grey50", linetype="dashed") + scale_color_brewer(palette="Set1") + scale_fill_brewer(palette="Set1") + scale_y_continuous(expand=c(0, 0), limits=c(0, NA)) + theme(aspect.ratio=1, legend.position="right")

ggplot(df, aes(Correlation, fill=Method, color=Method)) + geom_freqpoly(alpha=1, bins=200) + theme_classic() + geom_vline(xintercept=0, color="grey50", linetype="dashed") + scale_color_brewer(palette="Set1") + scale_fill_brewer(palette="Set1") + scale_y_continuous(expand=c(0, 0), limits=c(0, NA)) + theme(aspect.ratio=1, legend.position="right") 

ggplot(df, aes(Correlation, fill=Method, color=Method)) + geom_freqpoly(alpha=1, bins=200) + theme_classic() + geom_vline(xintercept=0, color="grey50", linetype="dashed") + scale_color_brewer(palette="Set1") + scale_fill_brewer(palette="Set1") + scale_y_log10(expand=c(0, 0), limits=c(1000, NA)) + theme(aspect.ratio=1, legend.position="right") 
```




When p >> n, the estimate of $\hat\Sigma$ is poor, even with shrinkage.  In this case `eclairs + decorrelate` does reduce the correlation, but the amount varies.  However, this is not an issue because in regression, the covariance matrix is actually known.  Can I prove this?

Since $\lambda \rightarrow 0$ as $n \rightarrow \infty$ regression after ADT is asymptotically consistent



n_features = 500
# X = Y[1:1,1:n_features]

X = Y[1:500,]

C1 = cora(X)
C2 = cora(whiten(X))
C3 = cora(whiten(whiten(X)))
C4 = cora(whiten(whiten(whiten(X))))


par(mfrow=c(1,3))
image(C1[1:100, 1:100])
image(C2[1:100, 1:100])
image(C3[1:100, 1:100])



C1[1:3, 1:3]
C2[1:3, 1:3]
C3[1:3, 1:3]
C4[1:3, 1:3]

ecl = eclairs(X)
X2 = decorrelate(X, ecl)

ecl2 = eclairs(X2, lambda=ecl$lambda, compute="corr")
X3 = decorrelate(X2, ecl2)

cora(X3[,1:3])








